# Simple Linear Regression

\* By far the most common approach involves minimizing the \*least squares criterion\*.

\* Let $$\hat y\_i = \vec \beta\_0 + \vec \beta\_1x\_1$$ be prediction for $Y$ $$x = y$$ based on the $i$th value of $X$.

\* Then $e\_i = y\_i - \hat y\_i$ represents the $i$th \*redisual\*

\* We define the residual sum of squares\(RSS\) as
$$

RSS = e\_1^2 + e\_2^2 + ... + e\_n^2

$$

or equivalently as:

$$

RSS = \(y\_1 - \hat y\_1\)^2 + \(y\_2 - \hat y\_2\)^2 +  ... + \(\(y\_n - \hat y\_n\)^2

$$


