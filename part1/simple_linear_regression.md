# Simple Linear Regression

\* By far the most common approach involves minimizing the** least squares criterion**.

\* Let $$\hat y_i = \vec \beta_0 + \vec \beta_1x_1$$ be prediction for $$Y$$ $$x = y$$ based on the $$i$$th value of $$X$$.

\* Then $$e_i = y_i - \hat y_i$$ represents the $$i$$th **redisual**

\* We define the residual sum of squares\(RSS\) as


$$
RSS = e\_1^2 + e\_2^2 + ... + e\_n^2
$$


or equivalently as:


$$
RSS = (y_1 - \hat y_1)^2 + (y_2 - \hat y_2)^2 +  ... + (y_n - \hat y_n)^2
$$


