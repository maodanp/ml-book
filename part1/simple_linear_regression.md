# Simple Linear Regression

\* By far the most common approach involves minimizing the \*least squares criterion\*.

\* Let $$x = y$$$$x = y$$$\hat y\_i = \vec \beta\_0 + \vec \beta\_1x\_1$ be prediction for $Y$ based on the $i$th value of $X$.

\* Then $e\_i = y\_i - \hat y\_i$ represents the $i$th \*redisual\*

\* We define the residual sum of squares\(RSS\) as

```
$$

RSS = e\_1^2 + e\_2^2 + ... + e\_n^2

$$

or equivalently as:

$$

RSS = \(y\_1 - \hat y\_1\)^2 + \(y\_2 - \hat y\_2\)^2 +  ... + \(\(y\_n - \hat y\_n\)^2

$$
```


$$
x^2 = y^2
$$
$$x = y$$$$x = y$$

